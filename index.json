[{"categories":["Ansible","Full course"],"contents":"When we use ansible we often need to manage remote hosts, those hosts can be bare-metals, VMs, instances running on different clouds and so on. Before we start writing playbooks and tasks (will be covered later) we need to understand what we infrastructure we have and how should we manage it, this will have affect on how our playbooks will be structured. In this blog post we will talk about the ansible inventory, which is a file that list the hosts we manage and see cover some configurations that are relevant to the way we talk to our hosts.\nAnsible Inventory The ansible inventory is mostly a list of hosts that we want to manage with ansible. It allows us to easily group a number of hosts under the same name, and define variables for each host/group.\nIn ansible there are 2 types of inventories:\n static: a regular file which can be written in INI or YAML format. This is the most common and simple inventory and will be the main focus of this blog. dynamic: an executable script which returns a generated static inventory that reflects the current state of the infrastructure. This is a more advance type of inventory which will be covered briefly.  Inventory Structure The simplest inventory is just a file listing the hosts. For example this is a simple inventory that lists 5 hosts with their FQDN:\nhost1.example.gzaidman.com # dev host host2.example.gzaidman.com # test host3.example.gzaidman.com # test host4.example.gzaidman.com # prod host5.example.gzaidman.com # prod ** We could also use plain IP addresses, but as a best practice it is recommended pto use FQDN or or IP addresses with aliases.\nThe above could be shortened with ranges:\nhost1.example.gzaidman.com # dev host host[2:3].example.gzaidman.com # test host[4:5].example.gzaidman.com # prod When we run an ansible playbook with the above inventory, ansible will try to perform each task in the playbook on all the hosts in the inventory.\nHaving a simple inventory with 5 hosts is not elegant but is manageable, but what if we have 50? 100? 1000?? as our infrastructure grows bigger and the system becomes more complex it is clear that we need a better way to organize our hosts, thats where inventory group come into play.\nAnsible inventory groups are used to assign a label to a collection of hosts. For example, we can use groups in the above example:\n[dev] host1.example.gzaidman.com [test] host[2:3].example.gzaidman.com [prod] host[4:5].example.gzaidman.com Groups a single host can be members of multiple groups and a group can contain child groups with the :children syntax. Lets say we want to also group the hosts by their location:\n[dev] host1.example.gzaidman.com [test] host[2:3].example.gzaidman.com [prod] host[4:5].example.gzaidman.com [england] host1.example.gzaidman.com host3.example.gzaidman.com [finland] host2.example.gzaidman.com host4.example.gzaidman.com [us] host5.example.gzaidman.com [europe:children] england finland In ansible there are two default groups that are always created:\n all: contains every host in the inventory. ungrouped: contains all hosts that don’t have another group aside from all.  To verify that the inventory is ok, we can use:\n# ansible HOST/GROUP --list-hosts ansible canada --list-hosts Adding variables to managed hosts Very often we will want to define variables which are specific to a host or a group of hosts.\nWe can add the variables directly on each host\n[test] host1.example.gzaidman.com listen_port=4321 host2.example.gzaidman.com listen_port=8642 We can also add variables for a group:\n[test:vars] ntp_server=ntp.canada.example.com ntp_port=4567 proxy=proxy.canada.example.com Those variables are called host and group variables and they will be available in the playbook on the host. Host and group variables are very common but defining them directly on the inventory file can make it very verbose and hard to read especially when we have a large inventory file. Ansible lets us organize host and group variables on separate files which will make our inventory cleaner and our project more maintainable. We need to create host_vars and group_vars directories, those directories can contain a file with the name of the host/group where we will define all the variables. For example we can take the inventory above, and create:\n[gzaidman:example] tree . ├── group_vars │ └── test ├── host_vars │ ├── host1.example.gzaidman.com │ └── host2.example.gzaidman.com └── inventory The variable files should be in yaml syntax only which is a bit different from the above.\n[gzaidman:example]cattestntp_server:ntp.canada.example.comntp_port:4567proxy:proxy.canada.example.com[gzaidman:example]cathost1.example.gzaidman.comlisten_port:4321[gzaidman:example]cathost2.example.gzaidman.comlisten_port:8642You can also create directories named after your groups or hosts and place variable files in them. Ansible will read all the files in these directories in lexicographical order. An example with the \u0026lsquo;test\u0026rsquo; group:\n./example/group_vars/test/ntp_settings ./example/group_vars/test/proxy_settings\nThis can be very useful to keep your variables organized when a single file gets too big, or when you want to use Ansible Vault (covered later) on some group variables.\nInventory Location: The inventory can be in the following locations (ordered by precedence):\n parameter: ansible/ansible-playbook \u0026ndash;inventory PATHNAME or -i PATHNAME. Ansible configuration: with the inventory=INVENTORY_PATH. default location: /etc/ansible/host system\u0026rsquo;s default static inventory file.  Configurations There are a lot of flags we can set to control the way ansible connects to our hosts. We define does as inventory variables for a specific host/group. For the full list of configurations virsit the Docs.\nSome of the most common ones are:\n ansible_connection: The why ansible will try to connect to the host. Bt default ansible executes playbooks over SSH with a connection type called smart, but there are others connection types:  SSH Based: paramiko, ssh Non-SSH based: local, docker   ansible_host: The name of the host to connect to, if different from the alias you wish to give to it. ansible_port: The connection port number, if not the default (22 for ssh).  Examples To see examples on different inventories go to the docs on:\nhttps://docs.ansible.com/ansible/latest/user_guide/intro_inventory.html#inventory-setup-examples\nDynamic Inventory When working with numerous machines or in an environment where machines come and go very quickly, it can be hard to keep the static inventory files up-to-date. Ansible supports dynamic inventory scripts that retrieve current information from external sources, allowing the inventory to be created in real time based on the current env. These scripts collect information from an external source and create JSON format inventory files.\nIf the inventory file is executable, then it is treated as a dynamic inventory program and Ansible attempts to run it to generate the inventory. If the file is not executable, then it is treated as a static inventory.\nIf a dynamic inventory script does not exist for the infrastructure in use, you can write a custom dynamic inventory program. This is a link to the ansible doc for writing a dynamic inventory link. A few thing we should keep in mind:\n Can be written in any programming language, but must have an appropriate interpreter line (for example, #!/usr/bin/python). The script should return inventory information in JSON format. When passed the \u0026ndash;list option, the script must print a JSON-encoded hash/dictionary of all the hosts and groups in the inventory. When passed the the \u0026ndash;host HOST option, the script must print a JSON hash/dictionary consisting of variables which are associated with that host, or an empty JSON hash/dictionary.  Configuring Ansible On each ansible project it is recommended to create a configuration file to override the default configurations based on the project requirements. Ansible configuration file is located at /etc/ansible/ansible.cfg, to override some settings we need to create an project configuration file at the project root directory with the same name ansible.cfg. The stock configuration should be sufficient for most project, and you can examine the /etc/ansible/ansible.cfg file to see the default configuration you can override.\nThe Ansible configuration file consists of several sections, each section is inclosed with \u0026lsquo;[SECTION_NAME]\u0026rsquo;.\nWe will talk about two sections that relate to the way ansible connects to hosts.\nSome configurations which relate to host management (they can als be overriden at the playbook level):\n[defaults]\nThis is the first section in the file and it sets defaults for Ansible operation.\n inventory - Specifies the path to the inventory file. remote_user - specify a different remote user, by default it will try to log in with the user which ran the ansible command via SSH. ask_pass - Whether or not to prompt for an SSH password. Can be false if using SSH public key authentication.  [privilege_escalation]\nConfigures how Ansible performs privilege escalation on managed hosts\n become - Whether to automatically switch user on the managed host after connecting. This can also be specified by a play. become_method - How to switch user (default is sudo). become_user - The user to switch to on the managed host (default is root). become_ask_pass - Whether to prompt for a password for your become_method. Defaults to false.  We can see all the configuration options with the ansible-config list command.\n** It\u0026rsquo;s important to understand that the above options on the default section refer to the initial connections meaning how to connect to the host and the options on the privilege_escalation refer to what to do once you are connected.\nConfiguration File Precedence  ANSIBLE_CONFIG environment variable ./ansible.cfg ~/ansible.cfg /etc/ansible/ansible.cfg  The recommended practice is to create an ansible.cfg file in a directory from which you run Ansible commands (meaning option 2). We cab check which configuration file is being used with ansible \u0026ndash;version.\n","permalink":"https://examplesite.com/blog/ansible/1.-managing-hosts-with-ansible/","tags":["Ansible","FullCourse","GettingStarted"],"title":"Managing Hosts With Ansible"},{"categories":null,"contents":"Creating a Web Crawler with Colly, learn GO by doing Before we start This tutorial was created while implementing a small project that required web crawling. It is composed from various descriptions, definitions and examples I saw on the links in the References section as well as my own experience while working on the project.\nWhat is a web crawler? Essentially, a web crawler is a tool that inspects the HTML of a givin web page and performs some type of actions based on that content. On the simple case (which is what we will implement in this tutorial)the web crawler start from a simple page, and while scanning that page it acquire more links to visit. Lets look at the following pseudo code to better understand the basics:\nQueue = Queue() Queue.Add(initialURL) while Queue is not empty: URL = Queue.pop() Page = Visit(URL) Links = ExtractURLs(Page) for link in Links: Queue.Add(link) Usually when we think of writing wab crawlers the first language tha comes to mind is python, with its wide selection of frameworks and reach data processing abilities, however Golang has a very good rich ecosystem for web crawling as well that lets you utilize the efficiency of Golang.\nColly - The golang web crawling framework Introduction Colly is a Golang framework for building web scrapers. With Colly you can build web scrapers of various complexity, from simple scraper to complex asynchronous website crawlers processing millions of web pages. Colly is very much \u0026ldquo;Batteries-Included\u0026rdquo;, meaning you will get most the required features \u0026ldquo;Out of the box\u0026rdquo;. Colly has a rich API with features such as:\n Manages request delays and maximum concurrency per domain Automatic cookie and session handling Sync/async/parallel scraping Distributed scraping Caching Automatic encoding of non-unicode responses  To install Colly we need to have Golang installed and run:\ngo get -u github.com/gocolly/colly/... Then in our go file we need to import it:\nimport \u0026#34;github.com/gocolly/colly\u0026#34; Latest info can be found in colly installation guide\nBasic Components Collector Colly’s main entity is the Collector struct. The Collector keep track of pages that are queued to visit, manages the network communication and responsible for the execution of the attached callbacks when a page is being scraped.\nTo initialize a Collector we need to call the NewCollector function:\n// Create a collector with the default configuration c := colly.NewCollector() The NewCollector function definition is\nfunc NewCollector(options ...CollectorOption) *Collector CollectorOption is a function which accepts a pointer to a Collector and configures it\ntype CollectorOption func(*Collector) We can basically configure each field in the Collector struct, for example here is a collector that is Asynchronous and will will only go to links on the starting page:\nc := colly.NewCollector( // MaxDepth is 2, so only the links on the scraped page  // and links on those pages are visited  colly.MaxDepth(2), colly.Async(true), ) Important Notes:\n We can override our configuration at any point. We can use environment variables to configure our collector, this is useful if we don\u0026rsquo;t want to recompile the program for customizing the collector, but we need to remember that every configuration that is defined in our program will override our environment variable.  You can see the full list of CollectorOption functions in colly goDocs and for more detailed information about how we can configure the collector go the the colly configuration docs\nCallbacks Colly gives us a number of callbacks that we can setup. Those callbacks will be called on various stages in our crawling job and you will need to think which one do you want based on you requirements. Here is a list of all the callbacks and the order in which they will be called (taken from the colly website[1]):\n OnRequest(f RequestCallback) - Called before a request. OnError(f ErrorCallback) - Called if error occured during the request. OnResponse(f ResponseCallback) - Called after response received. OnHTML(goquerySelector string, f HTMLCallback) - Called right after OnResponse if the received content is HTML. OnXML(xpathQuery string, f XMLCallback) - Called right after OnHTML if the received content is HTML or XML. OnScraped(f ScrapedCallback) - Called after OnXML callbacks.  Each of those callbacks receive a function which will be triggered when the callback is called.\nNow that we understand the basics of colly, lets dive into our project\nOur project Overview So under the openshift organization we have various github repos. Merged PRs will trigger a release build. Whenever QE needs to verify a Bug which is resolved in a certain PR they need to manually look at when the PR was merged and then go to Openshift release page and find a release job which was triggered after the time the PR was merged, check its page and see that it contains a the PR and then they know that they can use the release for verification. We want to automate this task.\nWe will build a small project that will use github golang library for checking when a PR was merged, and then crawl on the openshift release page and find the release which contain the PR.\nLogic  Retrieve the PR from the user. Validate that it is a github PR link. Find when the PR was merged. Find all the links to release pages on the openshift release. On each link search for the PR ID.  Implementation So we will use:\n flags - for parsing our commend line args. regexp - to validate the pattern of a github repo. github api - to retrieve the PR data.  Our main function will look like:\nfunc main() { var err error debug := flag.Bool(\u0026#34;debug\u0026#34;, false, \u0026#34;run with debug\u0026#34;) flag.Parse() url := flag.Arg(0) prd, err = *createPullRequestData(url) if err != nil { panic(err) } setupReleasePageCrawler(*debug) setupReleaseStatusCrawler(*debug) releaseStatusCrawler.Visit(\u0026#34;https://\u0026#34; + OPENSHIFT_RELEASE_DOMAIN) releasePageCrawler.Wait() fmt.Println(\u0026#34;Done!\u0026#34;) } We get the URL from the user and also configure a flag for running in DEBUG mode. Then we get the PR details and configure our crawlers\n releaseStatusCrawler will be in charge of parsing the release domain and finding relevant release pages. releasePageCrawler will be in charge of finding the PR in the release page.  And at the end we visit the release page, and call the Wait function on releasePageCrawler since it will run asynchronously.\nFirst of all, lets create a struct that will hold all of the data which we need for a PR.\n// PullRequestData holds all the data needed for a PR type PullRequestData struct { org string repo string id int mDate time.Time } Then write a the logic for getting a PR Url and creating a PullRequestData object.\n// Gets a PR URL and returns the organization, repo and ID of that PR. // If the URL is not a valid PR url then returns an error func parsePrURL(prURL string) (string, string, int, error) { githubReg := regexp.MustCompile(`(https*:\\/\\/)?github\\.com\\/(.+\\/){2}pull\\/\\d+`) if !githubReg.MatchString(prURL) { return \u0026#34;\u0026#34;, \u0026#34;\u0026#34;, 0, fmt.Errorf(\u0026#34;error: PR URL is not a github PR URL, got url %v\u0026#34;, prURL) } u, err := url.Parse(prURL) if err != nil { return \u0026#34;\u0026#34;, \u0026#34;\u0026#34;, 0, err } pArr := strings.Split(u.Path, \u0026#34;/\u0026#34;) id, err := strconv.Atoi(pArr[4]) if err != nil { return \u0026#34;\u0026#34;, \u0026#34;\u0026#34;, 0, fmt.Errorf(\u0026#34;error: expected PR URL with the form of github.com/ORG/REPO/pull/ID, but got %v\u0026#34;, prURL) } return pArr[1], pArr[2], id, nil } // Creates a Github client using AccessToken if it exists or an un authenticated client // if no AccessToken is available and retrieves the PR details from github func getGithubPr(org string, repo string, id int) (*github.PullRequest, error) { var client *github.Client ctx := context.Background() accessToken := os.Getenv(\u0026#34;AccessToken\u0026#34;) if accessToken == \u0026#34;\u0026#34; { client = github.NewClient(nil) } else { ts := oauth2.StaticTokenSource( \u0026amp;oauth2.Token{AccessToken: accessToken}, ) tc := oauth2.NewClient(ctx, ts) client = github.NewClient(tc) } pr, _, err := client.PullRequests.Get(ctx, org, repo, id) return pr, err } func createPullRequestData(prURL string) (*PullRequestData, error) { org, repo, id, err := parsePrURL(prURL) if err != nil { return PullRequestData{}, err } pr, err := getGithubPr(org, repo, id) if err != nil { return PullRequestData{}, err } return \u0026amp;PullRequestData{ org: org, repo: repo, id: id, mDate: pr.GetMergedAt(), }, nil } Here we get a PR URL, then call parsePrURL which validates that the URL is correct and returns the organization, repository and id from the URL. After we retrieved the fields from the PR URL we call getGithubPr which creates a github client and uses github API for getting the PR details. Finally we return a pointer to a PullRequestData object.\nNow lets configure our first crawler, the release status crawler. The release status crawler works on one page only, the OCP release page which contains ~2000+ links.\nfunc setupReleaseStatusCrawler(debug bool) { releaseStatusCrawler = colly.NewCollector( colly.AllowedDomains(OPENSHIFT_RELEASE_DOMAIN), colly.MaxDepth(1), ) if debug == true { releaseStatusCrawler.OnRequest(func(r *colly.Request) { fmt.Println(\u0026#34;Debug: release status crawler is visiting: \u0026#34;, r.URL.String()) }) } releaseStatusCrawler.OnHTML(\u0026#34;a[href]\u0026#34;, filterReleasesLinks) } Here we create a new crawler that can visit only ocp domain and set MaxDepth to 1 so it will stay only one first link. Then we will use the OnRequest callback to print the URL we are visiting in debug mod. Finally we use the OnHTML callback to set a function that will be called on each a[href] object. filterReleasesLinks is a function of type HTMLCallback. It gets the a[href] object and triggers the releasePageCrawler (which works on async mode) if that element fits the release regex and is created after the PR is merged.\nfunc filterReleasesLinks(e *colly.HTMLElement) { reRelease := regexp.MustCompile(`\\d\\.\\d\\.\\d-\\d\\.(nightly|ci)-\\d{4}-\\d{2}-\\d{2}-\\d{6}`) if !reRelease.MatchString(e.Text) { return } d := strings.SplitN(e.Text, \u0026#34;-\u0026#34;, 3)[2] d = strings.Split(d, \u0026#34; \u0026#34;)[0] tr, err := time.Parse(PR_DATE_FORMAT, d) if err != nil { fmt.Printf(\u0026#34;Error time.Parse: \u0026#34;, err) } // Check if the release is created after the PR is merged \tif tr.mDate.After(prd) { link := e.Attr(\u0026#34;href\u0026#34;) releasePageCrawler.Visit(e.Request.AbsoluteURL(link)) } } Now lets configure our second crawler, the release page crawler. The release page crawler works on one page only, the release page which should contain a link the the given PR. The release page crawler will be triggered by the release status crawler when it finds a relevant link, so it needs to work on Async mode.\nfunc setupReleasePageCrawler(debug bool) { releasePageCrawler = colly.NewCollector( colly.AllowedDomains(OPENSHIFT_RELEASE_DOMAIN), colly.MaxDepth(1), colly.Async(true), ) //Set max Parallelism and introduce a Random Delay \treleasePageCrawler.Limit(\u0026amp;colly.LimitRule{ Parallelism: 6, RandomDelay: 1 * time.Second, }) if debug == true { releasePageCrawler.OnRequest(func(r *colly.Request) { fmt.Println(\u0026#34;Debug: release page crawler is visiting: \u0026#34;, r.URL.String()) }) } releasePageCrawler.OnHTML(\u0026#34;a[href]\u0026#34;, findPR) } Here we create a new crawler that can visit only ocp domain ,set MaxDepth to 1 so it will stay only one first link and configure it to work in Async mode. The we set a limit to be a good citizen of the web, this is important because websites can block users that overload the site. Then we will use the OnRequest callback to print the URL we are visiting in debug mod. Finally we use the OnHTML callback to set a function that will be called on each a[href] object. findPR is also a HTMLCallback function. It gets the a[href] object and if it is a valid PR URL checks if it has the same ID as the given PR. If the IDs match we know that we found a release which contains our PR and we print it for the user.\nfunc findPR(e *colly.HTMLElement) { link := e.Attr(\u0026#34;href\u0026#34;) _, _, id, err := parsePrURL(link) if err != nil { return } if prd.id == id { fmt.Println(\u0026#34;found release, link \u0026#34;, e.Request.URL.String()) } } The full code can be found on:\nReferences: [1] Colly web page and docs [2] Scraping the Web in Golang with Colly and Goquery\n","permalink":"https://examplesite.com/blog/golang/creating-a-web-crawler-with-colly-learn-go-by-doing/","tags":null,"title":""},{"categories":null,"contents":"JSON Marsheling and Unmarshaling Most of the communication that happens over REST involves passing data as JSON. JSON is a language-independent data format, but when we use it in our programs it is basically a string.We need to make sure that our JSON string is always valid JSON object this can be hard when we have a complex Struct that contains many fields. Marsheling and Unmarshaling objects helps us achieve that. Marsheling is the operation of converting Go objects into JSON strings. Unmarshalling is the operation of converting JSON strings into Go objects.\nThe first step is to add support for json to our struct:\ntype User struct { Fname string `json:\u0026#34;fname\u0026#34;` Lname string `json:\u0026#34;lname\u0026#34;` } We need to tell GO what is the name of the field in the JSON format and to which filed it correlates. To convert it to a JSON we use the json.Marshal method from the encoding/json package:\nfunc (u User) toJSON() string { byteArray, err := json.Marshal(book) if err != nil { // HANDLE  } return string(byteArray) } When we want to Unmarshal the object we use the json.Unmarshal method which receives a byte array containing the JSON and a pointer to the var object to populate:\nfunc Unmarshal(data []byte, v interface{}) error For example:\nfunc jsonToUser(j string) *User { var p User err := json.Unmarshal([]byte(jsonString), \u0026amp;p) if err != nil { // HANDLE  } return \u0026amp;p } Faster JSON marshal and unmarshal: The JSON marshaling and unmarshaling provided by the encoding/json package uses reflection to figure out values and types each time. Reflection, provided by the reflect package, takes time to figure out types and values each time a message is acted on (in Go is fairly fast). If you’re repeatedly acting on the same structures, quite a bit of time will be spent reflecting. Additionally, reflection allocates memory that needs to be garbage-collected, and there’s a small computational cost to that as well.\nThe solution is to use optimized generated code\nCodec The package github.com/ugorji/go/codec, provides a High Performance codec/encoding library for binc, msgpack, cbor, json formats.\nTo use it we need to define our stract with the codec tag (instead of JSON):\n// go:generate codecgen -o user_generated.go user.go  type User struct { Fname string `codec:\u0026#34;fname\u0026#34;` Lname string `codec:\u0026#34;lname\u0026#34;` } When we run:\ngo generate ./ go generate will see the first comment line of the file, which is specially formatted for it, and execute codecgen. The output file is named user_generated.go. In the generated file, you’ll notice that two public methods have been added to the User type:\n CodecEncodeSelf CodecDecodeSelf  When these are present, the codec package uses them to encode or decode the type. When they’re absent, the codec package falls back to doing these at runtime.\nfunc (u User) toJSON() string { jh := new(codec.JsonHandle) var out []byte err := codec.NewEncoderBytes(\u0026amp;out, jh).Encode() if err != nil { // HANDLE  } return string(byteArray) } func jsonToPerson(j string) *Person { jh := new(codec.JsonHandle) var p User err := json.NewDecoderBytes(j, jh).Decode(\u0026amp;p) if err != nil { // HANDLE  } return \u0026amp;p } Note you have to install: go get -u github.com/ugorji/go/codec/codecgen\nEasyjson TODO\nSources:  Go in Practice, Published by Manning Publications, 2016.   ","permalink":"https://examplesite.com/blog/golang/json-marshal-and-unmarshal/","tags":null,"title":""},{"categories":null,"contents":"Creating a Web Crawler with Colly, learn GO by doing Before we start This tutorial was created while implementing a small project that required web crawling. It is composed from various descriptions, definitions and examples I saw on the links in the References section as well as my own experience while working on the project.\nWhat is a web crawler? Essentially, a web crawler is a tool that inspects the HTML of a givin web page and performs some type of actions based on that content. On the simple case (which is what we will implement in this tutorial)the web crawler start from a simple page, and while scanning that page it acquire more links to visit. Lets look at the following pseudo code to better understand the basics:\nQueue = Queue() Queue.Add(initialURL) while Queue is not empty: URL = Queue.pop() Page = Visit(URL) Links = ExtractURLs(Page) for link in Links: Queue.Add(link) Usually when we think of writing wab crawlers the first language tha comes to mind is python, with its wide selection of frameworks and reach data processing abilities, however Golang has a very good rich ecosystem for web crawling as well that lets you utilize the efficiency of Golang.\nColly - The golang web crawling framework Introduction Colly is a Golang framework for building web scrapers. With Colly you can build web scrapers of various complexity, from simple scraper to complex asynchronous website crawlers processing millions of web pages. Colly is very much \u0026ldquo;Batteries-Included\u0026rdquo;, meaning you will get most the required features \u0026ldquo;Out of the box\u0026rdquo;. Colly has a rich API with features such as:\n Manages request delays and maximum concurrency per domain Automatic cookie and session handling Sync/async/parallel scraping Distributed scraping Caching Automatic encoding of non-unicode responses  To install Colly we need to have Golang installed and run:\ngo get -u github.com/gocolly/colly/... Then in our go file we need to import it:\nimport \u0026#34;github.com/gocolly/colly\u0026#34; Latest info can be found in colly installation guide\nBasic Components Collector Colly’s main entity is the Collector struct. The Collector keep track of pages that are queued to visit, manages the network communication and responsible for the execution of the attached callbacks when a page is being scraped.\nTo initialize a Collector we need to call the NewCollector function:\n// Create a collector with the default configuration c := colly.NewCollector() The NewCollector function definition is\nfunc NewCollector(options ...CollectorOption) *Collector CollectorOption is a function which accepts a pointer to a Collector and configures it\ntype CollectorOption func(*Collector) We can basically configure each field in the Collector struct, for example here is a collector that is Asynchronous and will will only go to links on the starting page:\nc := colly.NewCollector( // MaxDepth is 2, so only the links on the scraped page  // and links on those pages are visited  colly.MaxDepth(2), colly.Async(true), ) Important Notes:\n We can override our configuration at any point. We can use environment variables to configure our collector, this is useful if we don\u0026rsquo;t want to recompile the program for customizing the collector, but we need to remember that every configuration that is defined in our program will override our environment variable.  You can see the full list of CollectorOption functions in colly goDocs and for more detailed information about how we can configure the collector go the the colly configuration docs\nCallbacks Colly gives us a number of callbacks that we can setup. Those callbacks will be called on various stages in our crawling job and you will need to think which one do you want based on you requirements. Here is a list of all the callbacks and the order in which they will be called (taken from the colly website[1]):\n OnRequest(f RequestCallback) - Called before a request. OnError(f ErrorCallback) - Called if error occured during the request. OnResponse(f ResponseCallback) - Called after response received. OnHTML(goquerySelector string, f HTMLCallback) - Called right after OnResponse if the received content is HTML. OnXML(xpathQuery string, f XMLCallback) - Called right after OnHTML if the received content is HTML or XML. OnScraped(f ScrapedCallback) - Called after OnXML callbacks.  Each of those callbacks receive a function which will be triggered when the callback is called.\nNow that we understand the basics of colly, lets dive into our project\nOur project Overview So under the openshift organization we have various github repos. Merged PRs will trigger a release build. Whenever QE needs to verify a Bug which is resolved in a certain PR they need to manually look at when the PR was merged and then go to Openshift release page and find a release job which was triggered after the time the PR was merged, check its page and see that it contains a the PR and then they know that they can use the release for verification. We want to automate this task.\nWe will build a small project that will use github golang library for checking when a PR was merged, and then crawl on the openshift release page and find the release which contain the PR.\nLogic  Retrieve the PR from the user. Validate that it is a github PR link. Find when the PR was merged. Find all the links to release pages on the openshift release. On each link search for the PR ID.  Implementation So we will use:\n flags - for parsing our commend line args. regexp - to validate the pattern of a github repo. github api - to retrieve the PR data.  Our main function will look like:\nfunc main() { var err error debug := flag.Bool(\u0026#34;debug\u0026#34;, false, \u0026#34;run with debug\u0026#34;) flag.Parse() url := flag.Arg(0) prd, err = *createPullRequestData(url) if err != nil { panic(err) } setupReleasePageCrawler(*debug) setupReleaseStatusCrawler(*debug) releaseStatusCrawler.Visit(\u0026#34;https://\u0026#34; + OPENSHIFT_RELEASE_DOMAIN) releasePageCrawler.Wait() fmt.Println(\u0026#34;Done!\u0026#34;) } We get the URL from the user and also configure a flag for running in DEBUG mode. Then we get the PR details and configure our crawlers\n releaseStatusCrawler will be in charge of parsing the release domain and finding relevant release pages. releasePageCrawler will be in charge of finding the PR in the release page.  And at the end we visit the release page, and call the Wait function on releasePageCrawler since it will run asynchronously.\nFirst of all, lets create a struct that will hold all of the data which we need for a PR.\n// PullRequestData holds all the data needed for a PR type PullRequestData struct { org string repo string id int mDate time.Time } Then write a the logic for getting a PR Url and creating a PullRequestData object.\n// Gets a PR URL and returns the organization, repo and ID of that PR. // If the URL is not a valid PR url then returns an error func parsePrURL(prURL string) (string, string, int, error) { githubReg := regexp.MustCompile(`(https*:\\/\\/)?github\\.com\\/(.+\\/){2}pull\\/\\d+`) if !githubReg.MatchString(prURL) { return \u0026#34;\u0026#34;, \u0026#34;\u0026#34;, 0, fmt.Errorf(\u0026#34;error: PR URL is not a github PR URL, got url %v\u0026#34;, prURL) } u, err := url.Parse(prURL) if err != nil { return \u0026#34;\u0026#34;, \u0026#34;\u0026#34;, 0, err } pArr := strings.Split(u.Path, \u0026#34;/\u0026#34;) id, err := strconv.Atoi(pArr[4]) if err != nil { return \u0026#34;\u0026#34;, \u0026#34;\u0026#34;, 0, fmt.Errorf(\u0026#34;error: expected PR URL with the form of github.com/ORG/REPO/pull/ID, but got %v\u0026#34;, prURL) } return pArr[1], pArr[2], id, nil } // Creates a Github client using AccessToken if it exists or an un authenticated client // if no AccessToken is available and retrieves the PR details from github func getGithubPr(org string, repo string, id int) (*github.PullRequest, error) { var client *github.Client ctx := context.Background() accessToken := os.Getenv(\u0026#34;AccessToken\u0026#34;) if accessToken == \u0026#34;\u0026#34; { client = github.NewClient(nil) } else { ts := oauth2.StaticTokenSource( \u0026amp;oauth2.Token{AccessToken: accessToken}, ) tc := oauth2.NewClient(ctx, ts) client = github.NewClient(tc) } pr, _, err := client.PullRequests.Get(ctx, org, repo, id) return pr, err } func createPullRequestData(prURL string) (*PullRequestData, error) { org, repo, id, err := parsePrURL(prURL) if err != nil { return PullRequestData{}, err } pr, err := getGithubPr(org, repo, id) if err != nil { return PullRequestData{}, err } return \u0026amp;PullRequestData{ org: org, repo: repo, id: id, mDate: pr.GetMergedAt(), }, nil } Here we get a PR URL, then call parsePrURL which validates that the URL is correct and returns the organization, repository and id from the URL. After we retrieved the fields from the PR URL we call getGithubPr which creates a github client and uses github API for getting the PR details. Finally we return a pointer to a PullRequestData object.\nNow lets configure our first crawler, the release status crawler. The release status crawler works on one page only, the OCP release page which contains ~2000+ links.\nfunc setupReleaseStatusCrawler(debug bool) { releaseStatusCrawler = colly.NewCollector( colly.AllowedDomains(OPENSHIFT_RELEASE_DOMAIN), colly.MaxDepth(1), ) if debug == true { releaseStatusCrawler.OnRequest(func(r *colly.Request) { fmt.Println(\u0026#34;Debug: release status crawler is visiting: \u0026#34;, r.URL.String()) }) } releaseStatusCrawler.OnHTML(\u0026#34;a[href]\u0026#34;, filterReleasesLinks) } Here we create a new crawler that can visit only ocp domain and set MaxDepth to 1 so it will stay only one first link. Then we will use the OnRequest callback to print the URL we are visiting in debug mod. Finally we use the OnHTML callback to set a function that will be called on each a[href] object. filterReleasesLinks is a function of type HTMLCallback. It gets the a[href] object and triggers the releasePageCrawler (which works on async mode) if that element fits the release regex and is created after the PR is merged.\nfunc filterReleasesLinks(e *colly.HTMLElement) { reRelease := regexp.MustCompile(`\\d\\.\\d\\.\\d-\\d\\.(nightly|ci)-\\d{4}-\\d{2}-\\d{2}-\\d{6}`) if !reRelease.MatchString(e.Text) { return } d := strings.SplitN(e.Text, \u0026#34;-\u0026#34;, 3)[2] d = strings.Split(d, \u0026#34; \u0026#34;)[0] tr, err := time.Parse(PR_DATE_FORMAT, d) if err != nil { fmt.Printf(\u0026#34;Error time.Parse: \u0026#34;, err) } // Check if the release is created after the PR is merged \tif tr.mDate.After(prd) { link := e.Attr(\u0026#34;href\u0026#34;) releasePageCrawler.Visit(e.Request.AbsoluteURL(link)) } } Now lets configure our second crawler, the release page crawler. The release page crawler works on one page only, the release page which should contain a link the the given PR. The release page crawler will be triggered by the release status crawler when it finds a relevant link, so it needs to work on Async mode.\nfunc setupReleasePageCrawler(debug bool) { releasePageCrawler = colly.NewCollector( colly.AllowedDomains(OPENSHIFT_RELEASE_DOMAIN), colly.MaxDepth(1), colly.Async(true), ) //Set max Parallelism and introduce a Random Delay \treleasePageCrawler.Limit(\u0026amp;colly.LimitRule{ Parallelism: 6, RandomDelay: 1 * time.Second, }) if debug == true { releasePageCrawler.OnRequest(func(r *colly.Request) { fmt.Println(\u0026#34;Debug: release page crawler is visiting: \u0026#34;, r.URL.String()) }) } releasePageCrawler.OnHTML(\u0026#34;a[href]\u0026#34;, findPR) } Here we create a new crawler that can visit only ocp domain ,set MaxDepth to 1 so it will stay only one first link and configure it to work in Async mode. The we set a limit to be a good citizen of the web, this is important because websites can block users that overload the site. Then we will use the OnRequest callback to print the URL we are visiting in debug mod. Finally we use the OnHTML callback to set a function that will be called on each a[href] object. findPR is also a HTMLCallback function. It gets the a[href] object and if it is a valid PR URL checks if it has the same ID as the given PR. If the IDs match we know that we found a release which contains our PR and we print it for the user.\nfunc findPR(e *colly.HTMLElement) { link := e.Attr(\u0026#34;href\u0026#34;) _, _, id, err := parsePrURL(link) if err != nil { return } if prd.id == id { fmt.Println(\u0026#34;found release, link \u0026#34;, e.Request.URL.String()) } } The full code can be found on:\nReferences: [1] Colly web page and docs [2] Scraping the Web in Golang with Colly and Goquery\n","permalink":"https://examplesite.com/blog/golang/learn-go-by-doing/","tags":null,"title":""},{"categories":null,"contents":"Protocol Buffers REST is a very common and widely used for end-user-facing APIs, but with the rise of microservices and the amount of communication between them network communication can be a bottleneck. The network bottleneck has become enough of an issue that companies that operate on a large scale, such as Google and Facebook, have innovated new technologies to speed up communication between microservices.\nPrerequisite   Install the compiler: https://developers.google.com/protocol-buffers/docs/downloads.html.\n  Install the Go protocol buffer plugin:\ngo get -u github.com/golang/protobuf/protoc-gen-go   Introduction to protocol buffers JSON and XML are commonly used to serialize data.\nDefinition: Serialization is the process of translating data structures or object state into a format that can be stored (for example, in a file or memory buffer) or transmitted (for example, across a network connection link) and reconstructed later. These formats are fairly easy to use and the transfer format is easy to read, but they’re not optimized for transport over a network or for serialization. Protocol buffers, by Google, are a popular choice for a high-speed transfer format. The data in the messages being transferred over the network is smaller than XML or JSON and it can be marshaled and unmarshaled faster than XML and JSON as well. The transfer method isn’t defined by protocol buffers. They can be transferred on a filesystem, using RPC, over HTTP, via a message queue, and numerous other ways.\nA protocol format is defined in a file. It contains the structure of the message and can be used to automatically generate the needed code. For example: user.proto.\npackage project message User { required string name = 1; required int32 id = 2; optional string email = 3; }  The \u0026lsquo;=1\u0026rsquo; is a unique tag in the binary encoding. We need to specify whether a field is required or optional. Because the protocol buffer is used to generate Go code, it’s recommended that it have its own package. In this Go package, the protocol buffer file and generated code can reside.  For more information on protocol bafflers definition we can look at the DOCs.\nTo compile the protocol buffer to code, we need to generate the code (from the same directory as the .proto file):\nprotoc -I=. --go_out=. ./user.proto # -I specifies the input source directory. # --go_out indicates where the generated Go source files will go. # ./user.proto is the name of the file to generate the source from. After the generated code has been created, it can be used to pass messages between a client and server.\nFor example: Server:\nimport ( \u0026#34;net/http\u0026#34; pb \u0026#34;example/userpb\u0026#34; \u0026#34;github.com/golang/protobuf/proto\u0026#34; ) func main() { http.HandleFunc(\u0026#34;/\u0026#34;, func(res http.ResponseWriter, req *http.Request) { .... body, err := proto.Marshal(user) ... res.Write(body) }) http.ListenAndServe(\u0026#34;:8080\u0026#34;, nil) } Client:\nres. err:= http.Get(\u0026#34;http://localhost:8080\u0026#34;) if err != nil { os.Exit(1) } defer res.Body.Close() b, err := io.ReadAll(res.Body) if err != nil { os.Exit(1) } var user pb.User err = proto.Unmarshal(b, \u0026amp;user) if err != nil { os.Exit(1) } .... For more information and examples go to the gotutorial\nCommunicating over RPC with protocol buffers REST has certain semantics it enforces, which include a path and an HTTP verb, and are resource-based. At times, the semantics of REST aren’t desired.\ngRPC (www.grpc.io) is an open source, high-performance RPC framework that can use HTTP/2 as a transport layer. It was developed at Google and has support many languages. gRPC uses code generation to create the messages and handle parts of the communication. This enables the communication to easily work between multiple languages, as the interfaces and messages are generated properly for each language. To define the messages and RPC calls, gRPC uses protocol buffers\nsyntax = \u0026#34;proto3\u0026#34; package example service Hello { rpc Say (HelloRequest) return (HelloResponse) {} } message HelloRequest { string name = 1; } message HelloResponse { string message= 1; } For more details on version 3 of protocol buffers, see Docs.\nTo generate the code we need to add \u0026ndash;go_out=plugins=grpc:. to the protoc command:\nprotoc -I=. --go_out=plugins=grpc:. ./hello.proto For example: Server:\nimport ( \u0026#34;net\u0026#34; \u0026#34;context\u0026#34; pb \u0026#34;example/hellopb\u0026#34; \u0026#34;google.golang.org/grpc\u0026#34; ) type server struct{} func (s *server) Say(ctx context.Context, in pb.HelloRequest) (*pb.HelloResponse, error) { m := \u0026#34;Hello\u0026#34; + in.Name return \u0026amp;pb.HelloResponse(Message: msg), nil } func main() { l, err := net.Listen(\u0026#34;txp\u0026#34;, \u0026#34;:55555\u0026#34;) s := grpc.NewServer() pb.RegisterHelloServer(s, \u0026amp;server) s.Serve(1) } When the client and server are using HTTP/2, as they are by default when both ends are using gRPC, the communications use the connection reuse and multiplexing. This enables a fast, modern transport layer for transferring the protocol buffer binary messages.\nThe advantages and disadvantages of RPC utilizing gRPC should be weighed before using them.\nThe advantages:\n Using protocol buffers, the payload size is smaller and faster to marshal or unmarshal than JSON or XML. The context allows for canceling, communicating timeouts or due dates, and other relevant information over the course of a remote call. The interaction is with a called procedure rather than a message that’s passed elsewhere. Semantics of the transport methodology—for example, HTTP verbs—don’t limit the communications over RPC.  The disadvantages:\n The transport payload isn’t human-readable as JSON or XML can be. Applications need to know the interface and details of the of the RPC calls. Knowing the semantics of the message isn’t enough. Integration is deeper than a message, such as you’d have with REST, because a remote procedure is being called. Exposing remote procedure access to untrusted clients may not be ideal and should be handled with care for security.  In general, RPCs can be a good alternative for interactions between microservices you control that are part of a larger service. They can provide fast, efficient communication and will even work across multiple programming languages. RPCs shouldn’t typically be exposed to clients outside your control, such as a public API.\nSources:  Go in Practice, Published by Manning Publications, 2016.   ","permalink":"https://examplesite.com/blog/golang/protocol-buffers/","tags":null,"title":""}]